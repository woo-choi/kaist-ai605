{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KAIST AI605 Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woo-choi/kaist-ai605/blob/main/KAIST_AI605_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbGnNWI1lRy_"
      },
      "source": [
        "# KAIST AI605 Assignment 1: Text Classification with RNNs\n",
        "Authors: Hyeong-Gwon Hong (honggudrnjs@kaist.ac.kr) and Minjoon Seo (minjoon@kaist.ac.kr)\n",
        "\n",
        "**Due Date:** March 31 (Wed) 11:00pm, 2021\n",
        "\n",
        "## Assignment Objectives\n",
        "- Verify theoretically and empirically why gating mechanism (LSTM, GRU) helps in Recurrent Neural Networks (RNNs)\n",
        "- Design an LSTM-based text classification model from scratch using PyTorch.\n",
        "- Apply the classification model to a popular classification task, Stanford Sentiment Treebank v2 (SST-2).\n",
        "- Achieve higher accuracy by applying common machine learning strategies, including Dropout.\n",
        "- Utilize pretrained word embedding (e.g. GloVe) to leverage self-supervision over a large text corpus.\n",
        "- (Bonus) Use Hugging Face library (`transformers`) to leverage self-supervision via large language models.\n",
        "\n",
        "## Your Submission\n",
        "Your submission will be a link to a Colab notebook that has all written answers and is fully executable. You will submit your assignment via KLMS. Use in-line LaTeX (see below) for mathematical expressions. Collaboration among students is allowed but it is not a group assignment so make sure your answer and code are your own. Make sure to mention your collaborators in your assignment with their names and their student ids.\n",
        "\n",
        "## Grading\n",
        "The entire assignment is out of 100 points. There are four bonus questions with 10 points each (two bonus questions added on Mar 19). Your final score can be higher than 100 points.\n",
        "\n",
        "\n",
        "## Environment\n",
        "You will only use Python 3.7 and PyTorch 1.8, which is already available on Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYxEp1XMpxem",
        "outputId": "9cd8bb7f-4ef0-4671-d74f-69aa8e56e096"
      },
      "source": [
        "from platform import python_version\n",
        "import torch\n",
        "\n",
        "print(\"python\", python_version())\n",
        "print(\"torch\", torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python 3.7.10\n",
            "torch 1.8.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB7xyzIgnnkA"
      },
      "source": [
        "## 1. Limitations of Vanilla RNNs\n",
        "In Lecture 04 and 05, we saw how RNNs suffer from exploding or vanishing gradients. We mathematically showed that, if the recurrent relation is\n",
        "$$ \\textbf{h}_t = \\sigma (\\textbf{V}\\textbf{h}_{t-1} + \\textbf{U}\\textbf{x}_t + \\textbf{b}) $$\n",
        "then\n",
        "$$ \\frac{\\partial \\textbf{h}_t}{\\partial \\textbf{h}_{t-1}} = \\text{diag}(\\sigma' (\\textbf{V}\\textbf{h}_{t-1} + \\textbf{U}\\textbf{x}_t + \\textbf{b}))\\textbf{V}$$\n",
        "so\n",
        "$$\\frac{\\partial \\textbf{h}_T}{\\partial \\textbf{h}_1} \\propto \\textbf{V}^{T-1}$$\n",
        "which means this term will be very close to zero if the norm of $\\bf{V}$ is smaller than 1 and really big otherwise.\n",
        "\n",
        "**Problem 1.1** *(10 points)* Explain how exploding gradient can be mitigated if we use gradient clipping.\n",
        "\n",
        "**Problem 1.2** *(10 points)* Explain how vanishing gradient can be mitigated if we use LSTM. See the Lecture 04 and 05 slides for the definition of LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0AmoAT3wA1J"
      },
      "source": [
        "## 2. Creating Vocabulary from Training Data\n",
        "Creating the vocabulary is the first step for every natural language processing model. In this section, you will use Stanford Sentiment Treebank v2, a popular dataset for sentiment classification, to create your vocabulary.\n",
        "\n",
        "### Obtaining SST-2 via GLUE\n",
        "General Language Understanding Evaluation (GLUE) benchmark is a collection of tools for evaluating the performance of models across a diverse set of existing natural language understanding (NLU) tasks. See GLUE website (https://gluebenchmark.com/) and the GLUE paper (https://openreview.net/pdf?id=rJ4km2R5t7) for more details. GLUE provides an easy way to access the datasets, including SST-2.\n",
        "You can download SST-2 dataset by following the steps below:\n",
        "\n",
        "1. Clone GitHub repository:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK0S_VTJxds4",
        "outputId": "4963e000-9a76-4e47-a1b4-e88a3ba1aaf8"
      },
      "source": [
        "!git clone https://github.com/nyu-mll/GLUE-baselines.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GLUE-baselines'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 891 (delta 1), reused 2 (delta 0), pack-reused 886\u001b[K\n",
            "Receiving objects: 100% (891/891), 1.48 MiB | 14.54 MiB/s, done.\n",
            "Resolving deltas: 100% (610/610), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8eWbt2yxb3H"
      },
      "source": [
        "2. Download SST-2 only:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6drFIvgxxjgI",
        "outputId": "f08c1cce-9184-40f4-bbc2-e1c041545100"
      },
      "source": [
        "%cd GLUE-baselines/\n",
        "!python download_glue_data.py --data_dir glue_data --tasks SST"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/GLUE-baselines\n",
            "Downloading and extracting SST...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAVQyYWkxib6"
      },
      "source": [
        "Your training, dev, and test data can be found at `glue_data/SST-2`. Note that each file is in a tsv format, where the first column is the sentence and the second column is the label (either 0 or 1, where 1 means positive sentiment). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElsgL-Piz7ck",
        "outputId": "5f5abe93-280f-43a8-981b-64804a714750"
      },
      "source": [
        "!head -10 glue_data/SST-2/train.tsv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence\tlabel\n",
            "hide new secretions from the parental units \t0\n",
            "contains no wit , only labored gags \t0\n",
            "that loves its characters and communicates something rather beautiful about human nature \t1\n",
            "remains utterly satisfied to remain the same throughout \t0\n",
            "on the worst revenge-of-the-nerds clich√©s the filmmakers could dredge up \t0\n",
            "that 's far too tragic to merit such superficial treatment \t0\n",
            "demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \t1\n",
            "of saucy \t1\n",
            "a depressed fifteen-year-old 's suicidal poetry \t0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s0T6qk6x78s"
      },
      "source": [
        "**Problem 2.1** *(10 points)* Using space tokenizer, create the vocabulary for the training data and report the vocabulary size here. Make sure that you add an `UNK` token to the vocabulary to account for words (during inference time) that you haven't seen. See below for an example with a short text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOsxkEpyxTW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6f3506-8d98-458d-fd09-c5ff47f85948"
      },
      "source": [
        "# Space tokenization\n",
        "text = \"Hello world!\"\n",
        "tokens = text.split(' ')\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'world!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iCTn95_pK7i",
        "outputId": "e2c159b4-c5a4-4d67-b9c7-da23466627e3"
      },
      "source": [
        "# Constructing vocabulary with `UNK`\n",
        "vocab = ['UNK'] + list(set(text.split(' ')))\n",
        "word2id = {word: id_ for id_, word in enumerate(vocab)}\n",
        "print(vocab)\n",
        "print(word2id['Hello'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['UNK', 'Hello', 'world!']\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqVv57zy1OZ0"
      },
      "source": [
        "**Problem 2.2** *(10 points)* Using all words in the training data will make the vocabulary very big. Reduce its size by only including words that occur at least 2 times. How does the size of the vocabulary change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDQbmM3W2Im3"
      },
      "source": [
        "## 3. Text Classification Baselines\n",
        "\n",
        "You can now use the vocabulary constructed from the training data to create an embedding matrix. You will use the embedding matrix to map each input sequence of tokens to a list of embedding vectors. One of the simplest baseline is to go through one layer of neural network and then average the outputs, and finally classify the average embedding: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFIAvGS5pQXC",
        "outputId": "a740c3d7-06dc-449d-b815-f767f495ff2a"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "input_ = \"hi world!\"\n",
        "input_tokens = input_.split(' ')\n",
        "input_ids = [word2id[word] if word in word2id else 0 for word in input_tokens]\n",
        "input_tensor = torch.LongTensor([input_ids]) # the first dimension is minibatch size\n",
        "print(input_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 2]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vUmITCFqMit",
        "outputId": "199fb8c2-3dbf-47c6-9f82-7fac1ef2c23a"
      },
      "source": [
        "# One layer, average pooling and classification\n",
        "class Baseline(nn.Module):\n",
        "  def __init__(self, d):\n",
        "    super(Baseline, self).__init__()\n",
        "    self.embedding = nn.Embedding(len(vocab), d)\n",
        "    self.layer = nn.Linear(d, d, bias=True)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.class_layer = nn.Linear(d, 2, bias=True)\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    emb = self.embedding(input_tensor)\n",
        "    out = self.relu(self.layer(emb))\n",
        "    avg = out.mean(1)\n",
        "    logits = self.class_layer(avg)\n",
        "    return logits\n",
        "\n",
        "d = 3 # usually bigger, e.g. 128\n",
        "baseline = Baseline(d)\n",
        "logits = baseline(input_tensor)\n",
        "softmax = nn.Softmax(1)\n",
        "print(softmax(logits)) # probability for each class"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.4113, 0.5887]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9UuMWCG9YNs"
      },
      "source": [
        "Now we will compute the loss, which is the negative log probability of the input text's label being the target label (`1`), which in fact turns out to be equivalent to the cross entropy (https://en.wikipedia.org/wiki/Cross_entropy) between the probability distribution and a one-hot distribution of the target label (note that we use `logits` instead of `softmax(logits)` as the input to the cross entropy, which allow us to avoid numerical instability). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nxgYNzQqaPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af74ddfb-8608-427e-8705-aaa91c50ffb8"
      },
      "source": [
        "cel = nn.CrossEntropyLoss()\n",
        "label = torch.LongTensor([1]) # The ground truth label for \"hi world!\" is positive.\n",
        "loss = cel(logits, label) # Loss, a.k.a L\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.5299, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKR99jZ2-wZW"
      },
      "source": [
        "Once we have the loss defined, only one step remains! We compute the gradients of parameters with respective to the loss and update. Fortunately, PyTorch does this for us in a very convenient way. Note that we used only one example to update the model, which is basically a Stochastic Gradient Descent (SGD) with minibatch size of 1. A recommended minibatch size in this exercise is at least 16. It is also recommended that you reuse your training data at least 10 times (i.e. 10 *epochs*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8JjhgQ071d6"
      },
      "source": [
        "optimizer = torch.optim.SGD(baseline.parameters(), lr=0.1)\n",
        "optimizer.zero_grad() # reset process\n",
        "loss.backward() # compute gradients\n",
        "optimizer.step() # update parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8M0fhFf_LbG"
      },
      "source": [
        "Once you have done this, all weight parameters will have `grad` attributes that contain their gradients with respect to the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpwOavsD8mpn",
        "outputId": "9bfa8f6d-a7e5-4e09-d6fc-eddf3d434a42"
      },
      "source": [
        "print(baseline.layer.weight.grad) # dL/dw of weights in the linear layer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0549, -0.4954,  0.5156],\n",
            "        [-0.0018,  0.0163, -0.0170],\n",
            "        [ 0.0000,  0.0000,  0.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-O9D26U_aEB"
      },
      "source": [
        "**Problem 3.1** *(10 points)* Properly train this average-pooling baseline model on SST-2 and report the model's accuracy on the dev data.\n",
        "\n",
        "**Problem 3.2** *(10 points)* Implement a recurrent neural network (without using PyTorch's RNN module) where the output of the linear layer not only depends on the current input but also the previous output. Report the model's accuracy on the dev data. Is it better or worse than the baseline? Why?\n",
        "\n",
        "**Problem 3.3 (bonus)** *(10 points)* Show that the cross entropy computed above is equivalent to the negative log likelihood of the probability distribution.\n",
        "\n",
        "**Problem 3.4 (bonus)** *(10 points)* Why is it numerically unstable if you compute log on top of softmax?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBLmpKJRAd1h"
      },
      "source": [
        "## 4. Text Classification with LSTM and Dropout\n",
        "\n",
        "Now it is time to improve your baselines! Replace your RNN module with an LSTM module. See Lecture slides 04 and 05 for the formal definition of LSTMs. \n",
        "\n",
        "You will also use Dropout, which randomly makes each dimension zero with the probability of `p` and scale it by `1/(1-p)` if it is not zero during training. Put it either at the input or the output of the LSTM to prevent it from overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8H0F-csCCk0",
        "outputId": "37826658-1f91-4b45-fa60-b1c332c23cc8"
      },
      "source": [
        "a = torch.FloatTensor([0.1, 0.3, 0.5, 0.7, 0.9])\n",
        "dropout = nn.Dropout(0.5) # p=0.5\n",
        "print(dropout(a))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.2000, 0.6000, 0.0000, 0.0000, 0.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6boSxKjC4Mw"
      },
      "source": [
        "**Problem 4.1** *(20 points)* Implement and use LSTM (without using PyTorch's LSTM module) instead of vanilla RNN to improve your model. Report the accuracy on the dev data.\n",
        "\n",
        "**Problem 4.2** *(10 points)* Use Dropout on LSTM (either at input or output). Report the accuracy on the dev data and briefly describe how it differs from 4.1.\n",
        "\n",
        "**Problem 4.3 (bonus)** *(10 points)* Consider implementing bidirectional LSTM and two layers of LSTM to further improve your model. Report your accuracy on dev data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "855DrT78DXps"
      },
      "source": [
        "## 5. Pretrained Word Vectors\n",
        "The last step is to use pretrained vocabulary and word vectors. The prebuilt vocabulary will replace the vocabulary you built with SST-2 training data, and the word vectors will replace the embedding vectors. You will observe the power of leveraging self-supservised pretrained models.\n",
        "\n",
        "**Problem 5.1** *(10 points)* Go to https://nlp.stanford.edu/projects/glove/ and download `glove.6B.zip`. Use these pretrained word vectors to further improve your model from 4.2. Report the model's accuracy on the dev data.\n",
        "\n",
        "**Problem 5.2 (bonus)** *(10 points)* You can go one step further by using word vectors obtained from pretrained language models. Can you import the word embeddings from `bert-base-uncased` model (via Hugging Face's `transformers`: https://huggingface.co/transformers/pretrained_models.html) into your model and improve it further? Report the accuracy on the dev data here. If the score is now higher, explain where the improvement is coming from."
      ]
    }
  ]
}